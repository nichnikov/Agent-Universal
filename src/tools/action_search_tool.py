"""
Tool for searching internal knowledge base (Action).
Implementation based on provided scripts and requirements.
"""

import logging
import os
import asyncio
from typing import Type, Optional, List, Dict, Any
from pydantic import BaseModel, Field, PrivateAttr
from langchain_core.tools import BaseTool

from .action_internal.client import SearchClient, SearchParams, SearchResult, DOC_API_URL
from .action_internal.json_parser import JsonDocumentParser
from .action_internal.xml_parser import XmlDocumentParser

logger = logging.getLogger(__name__)



class KnowledgeSearchInput(BaseModel):
    """Input schema for the Knowledge Search Tool."""
    query: Optional[str] = Field(
        None, 
        description="Single search query. Use this OR 'queries'."
    )
    queries: Optional[List[str]] = Field(
        None,
        description="List of search queries to run in parallel. Use this for comprehensive coverage. The number of queries should be specified in the prompt instructions."
    )
    limit: Optional[int] = Field(
        None, 
        description="Maximum number of documents to retrieve per query. This value should be specified in the prompt instructions and will be passed from there."
    )
    pub_alias: Optional[str] = Field(
        None,
        description="Optional publication alias filter."
    )


class KnowledgeSearchTool(BaseTool):
    """
    Tool for searching specific technical documentation, articles, and guidelines 
    within the company's internal knowledge base.
    Returns the content of relevant documents.
    """
    name: str = "internal_knowledge_search"
    description: str = (
        "Useful for searching specific technical documentation, articles, and guidelines "
        "within the company's internal knowledge base. "
        "Returns the content of relevant documents."
    )
    args_schema: Type[BaseModel] = KnowledgeSearchInput
    
    # Private attributes for internal services
    _client: SearchClient = PrivateAttr()
    _json_parser: JsonDocumentParser = PrivateAttr()
    _xml_parser: XmlDocumentParser = PrivateAttr()
    _default_pubdivid: int = PrivateAttr()
    _last_search_results: Optional[Dict[str, Any]] = PrivateAttr(default=None)

    def __init__(self, client: Optional[SearchClient] = None, default_pubdivid: int = 13):
        super().__init__()
        self._client = client or SearchClient()
        self._json_parser = JsonDocumentParser()
        self._xml_parser = XmlDocumentParser()
        self._default_pubdivid = default_pubdivid
        self._last_search_results = None

    async def _execute_single_search(self, query: str, limit: int, pub_alias: Optional[str]) -> Dict[str, Any]:
        """
        Executes a single search query and returns structured results.
        """
        logger.info(f"Executing search query: '{query}' with pubdivid={self._default_pubdivid}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–∏—Å–∫–∞
        params = SearchParams(
            fstring=query,
            pubAlias=pub_alias or "bss", # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç –∏–∑ —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            pubdivid=self._default_pubdivid, # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π pubdivid
            page=1,
            sortby="Relevance" 
        )

        # –í—ã–∑–æ–≤ –∫–ª–∏–µ–Ω—Ç–∞
        results: List[SearchResult] = await self._client.fetch_search_pages_and_docs(
            search_params=params,
            pages=1,
            base_search_url="https://1gl.ru/system/content/search-new/" # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π URL –ø–æ–∏—Å–∫–∞
        )

        structured_docs = []

        if results:
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-N —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            for res in results[:limit]:
                if res.error:
                    logger.warning(f"Error fetching doc {res.item.id}: {res.error}")
                    continue
                
                if not res.document:
                    continue

                # –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤
                is_xml_gateway = res.item.pubdivid in [3, 13]
                
                parsed_text = ""
                title = res.item.docName or "Untitled"

                try:
                    if is_xml_gateway:
                        # XML Parser logic
                        xml_title = self._xml_parser.get_title(res.document)
                        if xml_title:
                            title = xml_title
                        parsed_text = self._xml_parser.parse(res.document)
                    else:
                        # JSON Parser logic
                        parsed_text = self._json_parser.parse(res.document)
                except Exception as e:
                    logger.error(f"Error parsing document {res.item.id}: {e}")
                    continue

                # –û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è LLM
                MAX_CHARS = 4000
                if len(parsed_text) > MAX_CHARS:
                    parsed_text = parsed_text[:MAX_CHARS] + "\n...[Content Truncated]..."

                structured_docs.append({
                    "title": title,
                    "url": res.item.url,
                    "content": parsed_text,
                    "source_id": res.item.id,
                    "module_id": res.item.moduleId
                })
            
        return {
            "query": query,
            "documents": structured_docs
        }

    async def _arun(self, query: Optional[str] = None, queries: Optional[List[str]] = None, limit: Optional[int] = None, pub_alias: Optional[str] = None) -> str:
        """
        Executes the search and returns formatted document contents.
        Supports single 'query' or multiple 'queries'.
        """
        logger.info(f"Tool '{self.name}' called with query='{query}', queries='{queries}'")

        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
            search_queries = []
            if queries:
                search_queries = queries
            elif query:
                search_queries = [query]
            
            if not search_queries:
                return "Error: No search queries provided. Please provide 'query' or 'queries'."

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ limit –∏–∑ –ø—Ä–æ–º–ø—Ç–∞, –µ—Å–ª–∏ –æ–Ω–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —É–∫–∞–∑–∞–Ω–æ –≤ –ø—Ä–æ–º–ø—Ç–µ, –Ω–æ –Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º 5
            effective_limit = limit if limit is not None else 5
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            tasks = [self._execute_single_search(q, effective_limit, pub_alias) for q in search_queries]
            results_list = await asyncio.gather(*tasks)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            all_formatted_outputs = []
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø–æ–∏—Å–∫–∞
            metadata_header = f"SEARCH METADATA:\nQueries: {search_queries}\nLimit per query: {effective_limit}\nPubDivID: {self._default_pubdivid}\n---\n"
            all_formatted_outputs.append(metadata_header)

            for res in results_list:
                q = res['query']
                docs = res['documents']
                
                if not docs:
                    all_formatted_outputs.append(f"Search Query: {q}\nNo documents found.")
                    continue

                doc_strings = []
                for d in docs:
                    doc_entry = (
                        f"Title: {d['title']}\n"
                        f"URL: {d['url']}\n"
                        f"Content:\n{d['content']}\n"
                    )
                    doc_strings.append(doc_entry)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –±–ª–æ–∫ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
                docs_block = "\n---\n".join(doc_strings)
                all_formatted_outputs.append(f"Search Query: {q}\nResults:\n{docs_block}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ Langfuse
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: [–û—Ç–≤–µ—Ç 1, –û—Ç–≤–µ—Ç 2, ...]
            structured_results = {}
            for res in results_list:
                query = res['query']
                docs = res['documents']
                structured_results[query] = [
                    {
                        "title": doc['title'],
                        "url": doc['url'],
                        "content": doc['content']
                    }
                    for doc in docs
                ]
            
            self._last_search_results = structured_results

            if len(all_formatted_outputs) <= 1: # –¢–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                return "No documents found matching your queries."

            return "\n\n====================\n\n".join(all_formatted_outputs)

        except Exception as e:
            logger.error(f"Tool execution failed: {e}", exc_info=True)
            return f"Error executing search: {str(e)}"

    def _run(self, *args, **kwargs):
        raise NotImplementedError("This tool only supports async execution. Please use ainvoke() or async agent.")
    
    def get_last_search_results(self) -> Optional[Dict[str, Any]]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞.
        –§–æ—Ä–º–∞—Ç: {–ø–æ–∏—Å–∫–æ–≤—ã–π_–∑–∞–ø—Ä–æ—Å: [{"title": ..., "url": ..., "content": ...}, ...]}
        """
        return self._last_search_results


def create_search_tool(default_pubdivid: int = 13) -> KnowledgeSearchTool:
    """Factory function to create the search tool."""
    client = SearchClient()
    return KnowledgeSearchTool(client=client, default_pubdivid=default_pubdivid)


if __name__ == "__main__":
    import asyncio
    from dotenv import load_dotenv
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO)
    
    async def main():
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        # –ó–∞–ø—É—Å–∫: python -m src.tools.action_search_tool
        load_dotenv()
        print("üîç –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ KnowledgeSearchTool...")
        
        try:
            # –°–æ–∑–¥–∞–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
            tool = create_search_tool(1)
            
            # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤)
            queries = ["–∫–æ–≥–¥–∞ —É–ø—Ä–æ—â–µ–Ω–µ—Ü –ø–ª–∞—Ç–∏—Ç –ù–î–°", "–ù–î–° —Å—Ä–æ–∫–∏ —É–ø–ª–∞—Ç—ã"]
            limit = 2
            
            print(f"\n–ó–∞–ø—Ä–æ—Å—ã: '{queries}' (limit={limit})")
            
            # –í—ã–∑—ã–≤–∞–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
            result = await tool.ainvoke({"queries": queries, "limit": limit})
            
            print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç:\n{'-'*40}\n{result}\n{'-'*40}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ü–∏–∫–ª
    asyncio.run(main())
