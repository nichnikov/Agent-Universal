Это техническое задание (ТЗ) разработано для создания **унифицированной архитектуры инструментов (Tools)** для AI-агентов. В качестве эталонной реализации (Proof of Concept) используется сервис поиска по внутренней базе знаний, код которого предоставлен.

---

# ТЕХНИЧЕСКОЕ ЗАДАНИЕ: Реализация инструментов для AI-агентов (Agent Tools)

## 1. Общие положения
**Цель:** Разработать стандартизированный слой абстракции (Tooling Layer), позволяющий LLM-агентам взаимодействовать с внешними API.
**Принцип:** Инструмент должен быть атомарным, самодокументируемым, устойчивым к ошибкам и возвращать данные в формате, оптимизированном для контекстного окна LLM.

## 2. Архитектурные требования (Best Practices)

### 2.1. Паттерн "Tool Adapter"
Необходимо обернуть существующую бизнес-логику (`client.py`, `parsers`) в класс-адаптер, реализующий единый интерфейс.

*   **Входные данные:** Строгая типизация через Pydantic (для автоматической генерации JSON Schema для OpenAI functions / LangChain).
*   **Исполнение:** Асинхронное выполнение (asyncio), так как I/O операции (сеть) являются блокирующими.
*   **Выходные данные:** Текстовое представление (Markdown) или структурированный объект, очищенный от служебного шума JSON, чтобы экономить токены.

### 2.2. Требования к реализации
1.  **Dependency Injection:** Клиенты API (`SearchClient`) и парсеры должны внедряться в инструмент, а не создаваться внутри жестко.
2.  **Docstrings:** Обязательное наличие подробных docstrings у класса инструмента и метода `run`. Именно это описание использует LLM для принятия решения о вызове инструмента.
3.  **Error Handling:** Инструмент не должен "ронять" агента. Ошибки должны перехватываться и возвращаться в виде строки: *"Error: description"*, чтобы агент мог попробовать другой подход.
4.  **Context Optimization:** Сырой JSON/XML из API недопустим. Требуется агрегация и очистка текста (используя предоставленные парсеры).

---

## 3. Спецификация данных (Interface Definitions)

### 3.1. Входные параметры инструмента (`SearchInput`)
Класс должен наследоваться от `pydantic.BaseModel`.

| Поле | Тип | Обязательность | Описание (для LLM) |
|---|---|---|---|
| `query` | `str` | Да | Поисковый запрос на естественном языке. |
| `limit` | `int` | Нет (def: 3) | Максимальное количество документов для анализа. |
| `pub_id` | `int` | Нет (def: 9) | ID публикации (контекст поиска). |

### 3.2. Формат ответа
Инструмент возвращает строку (String) в формате Markdown, объединяющую найденные документы. Это позволяет агенту сразу цитировать источники.

**Шаблон ответа:**
```markdown
## Document: [Title] (ID: [id])
Source: [URL]
Content:
[Cleaned text content...]
---
```

---

## 4. Логика работы инструмента "KnowledgeSearchTool"

### 4.1. Инициализация
Инструмент принимает экземпляры:
*   `SearchClient` (для запросов).
*   `JsonDocumentParser` (для стандартных документов).
*   `XmlDocumentParser` (для документов из шлюза Internal Gateway).

### 4.2. Алгоритм выполнения (`_arun`)
1.  **Валидация:** Принять `query`. Если пустой — вернуть ошибку.
2.  **Поиск и Фетчинг:** Вызвать `SearchClient.fetch_search_pages_and_docs`.
    *   Параметры поиска: `q=query`, `page=1`.
    *   Фильтрация: брать только `limit` первых результатов.
3.  **Маршрутизация парсинга:**
    *   Для каждого `SearchResult`:
        *   Если `error` не пуст — пропустить или логировать.
        *   Проверить `pubdivid` или структуру `document`.
        *   Если документ пришел из Internal Gateway (pubdivid 3, 13) -> использовать `XmlDocumentParser`.
        *   Иначе -> использовать `JsonDocumentParser`.
4.  **Пост-обработка:**
    *   Извлечь заголовок (Title).
    *   Извлечь тело текста через метод `parse()`.
    *   Обрезать текст, если он превышает разумный лимит (например, 4000 символов), добавив маркер `[...truncated]`.
5.  **Сборка:** Объединить результаты в одну строку через разделители.

---

## 5. Пример реализации (Reference Implementation)

Ниже представлен код, который разработчики должны взять за основу. Он демонстрирует интеграцию предоставленных файлов в современный `BaseTool` (на примере паттерна, совместимого с LangChain/LlamaIndex).

```python
from typing import Type, Optional
from pydantic import BaseModel, Field
import logging

# Импорты из предоставленных файлов
from client import SearchClient, SearchParams, SearchResult
from json_parser import JsonDocumentParser
from xml_parser import XmlDocumentParser

logger = logging.getLogger(__name__)

# 1. Определение входных параметров для LLM
class KnowledgeSearchInput(BaseModel):
    """Input schema for the Knowledge Search Tool."""
    query: str = Field(
        ..., 
        description="Search query or question to find information in the internal knowledge base."
    )
    limit: int = Field(
        3, 
        description="Maximum number of documents to retrieve. Defaults to 3. Increase only if comprehensive search is needed."
    )
    pub_alias: str = Field(
        None,
        description="Optional publication alias filter."
    )

# 2. Абстракция инструмента (Generic Agent Tool Structure)
class BaseAgentTool:
    """Base class for agent tools implementing best practices."""
    name: str
    description: str
    args_schema: Type[BaseModel]

    async def arun(self, **kwargs) -> str:
        raise NotImplementedError

# 3. Реализация конкретного инструмента
class KnowledgeSearchTool(BaseAgentTool):
    name = "internal_knowledge_search"
    description = (
        "Useful for searching specific technical documentation, articles, and guidelines "
        "within the company's internal knowledge base. "
        "Returns the content of relevant documents."
    )
    args_schema = KnowledgeSearchInput

    def __init__(self, client: SearchClient):
        self._client = client
        self._json_parser = JsonDocumentParser()
        self._xml_parser = XmlDocumentParser()

    async def arun(self, query: str, limit: int = 3, pub_alias: str = None) -> str:
        """
        Executes the search and returns formatted document contents.
        """
        logger.info(f"Tool '{self.name}' called with query: '{query}'")

        try:
            # Подготовка параметров поиска
            # Примечание: предполагаем, что pubdivid определяется динамически или задан константой в рамках задачи
            # Для универсальности можно искать везде (pubdivid=None) или уточнять
            params = SearchParams(
                fstring=query,
                pubAlias=pub_alias,
                page=1,
                # Сортировка по релевантности по умолчанию
                sortby="Relevance" 
            )

            # Вызов клиента (используем существующую логику пагинации, но ограничиваем 1 страницей для скорости)
            results: list[SearchResult] = await self._client.fetch_search_pages_and_docs(
                search_params=params,
                pages=1,
                base_search_url="https://site-backend-ss.prod.ss.aservices.tech/api/v1/desktop/search" # URL вынесен в конфиг в реальном коде
            )

            if not results:
                return "No documents found matching your query."

            # Обработка результатов
            formatted_outputs = []
            
            # Берем только топ-N результатов
            for res in results[:limit]:
                if res.error:
                    logger.warning(f"Error fetching doc {res.item.id}: {res.error}")
                    continue
                
                if not res.document:
                    continue

                # Маршрутизация парсеров
                # Логика определения типа документа на основе pubdivid или структуры
                # Согласно client.py pubdivid 3 и 13 идут через шлюз (XML)
                is_xml_gateway = res.item.pubdivid in [3, 13]
                
                parsed_text = ""
                title = res.item.docName or "Untitled"

                if is_xml_gateway:
                    # XML Parser logic
                    # Парсер XML имеет метод get_title, попробуем уточнить заголовок
                    xml_title = self._xml_parser.get_title(res.document)
                    if xml_title:
                        title = xml_title
                    parsed_text = self._xml_parser.parse(res.document)
                else:
                    # JSON Parser logic
                    parsed_text = self._json_parser.parse(res.document)

                # Очистка и форматирование для LLM
                # Обрезаем слишком длинные тексты, чтобы не забить контекст
                MAX_CHARS = 4000
                if len(parsed_text) > MAX_CHARS:
                    parsed_text = parsed_text[:MAX_CHARS] + "\n...[Content Truncated]..."

                doc_entry = (
                    f"## Document: {title}\n"
                    f"Source ID: {res.item.id} (Module: {res.item.moduleId})\n"
                    f"URL: {res.item.url}\n"
                    f"Content:\n{parsed_text}\n"
                )
                formatted_outputs.append(doc_entry)

            return "\n---\n".join(formatted_outputs)

        except Exception as e:
            logger.error(f"Tool execution failed: {e}", exc_info=True)
            return f"Error executing search: {str(e)}"

# 4. Пример использования (фабричный метод)
def create_search_tool() -> KnowledgeSearchTool:
    client = SearchClient()
    # Здесь можно настроить таймауты или хэдеры клиента
    return KnowledgeSearchTool(client)
```

## 6. Рекомендации по интеграции

1.  **Асинхронность:** Убедитесь, что среда исполнения агента (Runtime) поддерживает `async/await`. Если используется синхронный фреймворк, потребуется обертка `asyncio.run()`, но это не рекомендуется для high-load.
2.  **Конфигурация URL:** URL-адреса API (`DOC_API_URL`, `INTERNAL_GATEWAY_API_URL`) должны быть вынесены в переменные окружения или конфиг-файл, а не зашиты в код клиента.
3.  **Токенизация:** В `parsed_text` рекомендуется добавить подсчет токенов (опционально), если используется модель с малым контекстом, чтобы динамически регулировать `limit`.
4.  **Логирование:** В `client.py` и парсерах уже есть логгеры. В инструменте необходимо логировать сам факт вызова и результат (успех/ошибка), но не полное тело ответа (GDPR/Security).